{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "import re\n",
                "import math\n",
                "import string\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn import decomposition, preprocessing\n",
                "from sklearn.metrics.pairwise import euclidean_distances\n",
                "%matplotlib inline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "def textFileToWordCounts(filename,minCount):\n",
                "\n",
                "    # read in document and create a vector of term counts\n",
                "    fileptr = open(filename)\n",
                "\n",
                "    # dictionary of words to counts\n",
                "    wordmap = {}\n",
                "\n",
                "    for line in fileptr:\n",
                "        for token in line.split(\" \"):\n",
                "            # convert to lowercase\n",
                "            lowertoken = token.lower()\n",
                "            # strip all non-character with regex\n",
                "            striptoken = re.sub(r\"[^a-z]\",\"\",lowertoken)\n",
                "            if len(striptoken)>0:\n",
                "                if striptoken not in wordmap.keys():\n",
                "                    wordmap[striptoken]=0\n",
                "                wordmap[striptoken]=wordmap[striptoken]+1\n",
                "    fileptr.close()\n",
                "    \n",
                "    # prune out words with fewer than minCount occurences\n",
                "    for token in list(wordmap.keys()):\n",
                "        if wordmap[token] < minCount:\n",
                "            del wordmap[token]\n",
                "    #return dictionary of counts\n",
                "    return wordmap\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "tmpWordMap = textFileToWordCounts('data\/purdue.txt',3)\n",
                "print(len(tmpWordMap))\n",
                "print(tmpWordMap)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "# calculate term vector for set of files\n",
                "directory = 'data'\n",
                "minThresh=3\n",
                "fileMap = {}\n",
                "uniqueWords = set()\n",
                "for filename in os.listdir(directory):\n",
                "    tmpWordMap = textFileToWordCounts(directory + '\/' + filename, minThresh)\n",
                "    #print(len(tmpWordMap))\n",
                "    fileMap[filename] = tmpWordMap\n",
                "    # add words to overall list of words\n",
                "    for tmpWord in tmpWordMap.keys():\n",
                "        uniqueWords.add(tmpWord)\n",
                "\n",
                "# create a document term matrix for the set of files\n",
                "fileList = list(fileMap.keys())\n",
                "numFiles = len(fileList)\n",
                "numWords = len(uniqueWords)\n",
                "uniqueWordList = list(uniqueWords)\n",
                "uniqueWordList.sort()\n",
                "print('Num words ' + str(numWords) + ', num files '+ str(numFiles))\n",
                "\n",
                "# initialize matrix to zeros\n",
                "termMatrix = np.zeros((numFiles,numWords))\n",
                "# loop over each wordmap\n",
                "for file in fileList:\n",
                "    rowIdx = fileList.index(file)\n",
                "    tmpWordMap = fileMap[file]\n",
                "    for token in tmpWordMap.keys():\n",
                "        # find location of word\n",
                "        colIdx = uniqueWordList.index(token)\n",
                "        # update count for appropriate cell of matrix\n",
                "        termMatrix[rowIdx,colIdx]=tmpWordMap[token]\n",
                "print(termMatrix[:6,:12])\n",
                "print(fileList)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "# visualize the data\n",
                "#plt.imshow(termMatrix)\n",
                "plt.imshow(termMatrix[:,:30])\n",
                "uniqueWordList[0]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "# reduce dimensionality with PCA\n",
                "def applyPCA(data,numCmps):\n",
                "    # mean center data\n",
                "    X_scaled = preprocessing.scale(data, with_std=False) \n",
                "    # initialize PCA model\n",
                "    pca = decomposition.PCA(n_components=numCmps)\n",
                "    # fit PCA model with scaled data \n",
                "    X_trans = pca.fit_transform(X_scaled)\n",
                "    # return transformed data and explained variance\n",
                "    return [X_trans, pca.explained_variance_ratio_]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "[transData, exvar] = applyPCA(termMatrix,numFiles)\n",
                "# determine best number of dimensions\n",
                "cmpIdx=range(1,numFiles+1)\n",
                "plt.scatter(cmpIdx,exvar)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "# plot transformed data, 1st two dimensions\n",
                "plt.scatter(transData[:,0],transData[:,1])\n",
                "# label points\n",
                "for file in fileList:\n",
                "    idx = fileList.index(file)\n",
                "    plt.text(transData[idx,0]+3,transData[idx,1]-2,file) "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "# normalize for document length\n",
                "normMatrix = termMatrix.copy()\n",
                "for i in range(termMatrix.shape[0]):\n",
                "    docLen = sum(termMatrix[i,:])\n",
                "    normMatrix[i,:] = termMatrix[i,:]\/docLen\n",
                "plt.imshow(normMatrix[:,:30])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "# apply PCA to normalized data\n",
                "[transNormData, exvar] = applyPCA(normMatrix,numFiles)\n",
                "\n",
                "plt.scatter(transNormData[:,0],transNormData[:,1])\n",
                "# label points\n",
                "for file in fileList:\n",
                "    idx = fileList.index(file)\n",
                "    plt.text(transNormData[idx,0],transNormData[idx,1],file+'('+str(idx)+')') "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "# calculate distances between documents\n",
                "dists = euclidean_distances(transNormData)\n",
                "plt.imshow(dists,cmap='RdBu')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "# multiply term frequency by inverse document frequency \n",
                "# IDF(t) = log(Num of docs \/ Num of docs with term t in it)\n",
                "tfidfMatrix = normMatrix.copy()\n",
                "for j in range(normMatrix.shape[1]):\n",
                "    docswterm = 0\n",
                "    for i in range(numFiles):\n",
                "        if normMatrix[i,j]>0:\n",
                "            docswterm = docswterm+1\n",
                "    termidf = 0\n",
                "    if docswterm > 0:\n",
                "        #print(str(j) + ':' + str(docswterm))\n",
                "        termidf = math.log(numFiles\/docswterm)\n",
                "    #print(str(j) + ':' + str(termidf))\n",
                "    tfidfMatrix[:,j] = normMatrix[:,j]*termidf\n",
                "plt.imshow(tfidfMatrix[:,:30])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "# apply PCA to normalized data\n",
                "[transTFIDFData, exvar] = applyPCA(tfidfMatrix,numFiles)\n",
                "\n",
                "plt.scatter(transTFIDFData[:,0],transTFIDFData[:,1])\n",
                "# label points\n",
                "for file in fileList:\n",
                "    idx = fileList.index(file)\n",
                "    plt.text(transTFIDFData[idx,0],transTFIDFData[idx,1],file) "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "# calculate distances between documents\n",
                "dists = euclidean_distances(transTFIDFData)\n",
                "plt.imshow(dists,cmap='RdBu')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "# find words with largest variance\n",
                "vars = []\n",
                "for j in range(tfidfMatrix.shape[1]):\n",
                "    tmpvar = np.var(tfidfMatrix[:,j])\n",
                "    vars.append(tmpvar)\n",
                "print(max(vars))\n",
                "thresh = np.percentile(vars,90)\n",
                "for j in range(tfidfMatrix.shape[1]):\n",
                "    if vars[j] > thresh:\n",
                "        print(uniqueWordList[j])\n",
                "        print(tfidfMatrix[:,j])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 [3.6]",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text\/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.4"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}