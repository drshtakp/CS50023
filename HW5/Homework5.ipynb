{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {
                "collapsed": true,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "# Import the necessary packages below\n",
                "\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import requests\n",
                "import string\n",
                "import re\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn import decomposition\n",
                "from sklearn import preprocessing\n",
                "from bs4 import BeautifulSoup\n",
                "%matplotlib inline"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "## Q1: Apply PCA to user-movie ratings"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "(a) Read in the data and transform data frames.\n",
                "\n",
                "i. Import the data from the csv file 'ratings.csv' into a pandas data frame. Each row corresponds to the rating given by a user to a particular movie. There is a header row that specifies the fields: userId, movieId, rating, timestamp. Transform the data into a movie-user ratings matrix such that rows refer to movies, columns refer to users, and each cell contains the rating for the associated movie\/user pair. Assign a value of 0 for any missing values. (Note: For this operation, you can use the pivot_table() function in pandas with index as movie ids, columns as user ids, values as rating.)\n",
                "\n",
                "ii. Import the data from the csv file 'movies.csv' into a pandas data frame. Each row corresponds to title and genre information for a particular movie. There is a header row that specifies the fields: movieId, title, genre. Each movie is associated with a list of genres, separated by '|'. Create a new column called 'firstGenre' that contains only the first genre from the list. You can do this easily with str.split().  Drop the original 'genre' column.\n",
                "\n",
                "iii. Return the data frames from (i) and (ii)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "# construct_data(ratingsFilename, genreFilename) takes as input the filenames to read \n",
                "# in data from (as specified above), converts the ratings data into a movie-user \n",
                "# dataframe df_A, and converts the movie data into a data frame df_B that includes a \n",
                "# 'firstGenre' column.\n",
                "# You will use these dataframes in subsequent questions. \n",
                "\n",
                "\n",
                "\n",
                "def construct_data(ratingsFilename, genreFilename):\n",
                "    \n",
                "    ###\n",
                "    ### YOUR CODE HERE\n",
                "    ###\n",
                "\n",
                "# Example: \n",
                "construct_data('ratings.csv', 'movies.csv') # -> (df_A 9724x610, df_B 9742x3)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {
                "collapsed": true,
                "nbgrader": {
                    "grade": true,
                    "grade_id": "Q1a:construct_dfA",
                    "locked": true,
                    "points": "2",
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "###\n",
                "### AUTOGRADER TEST - DO NOT REMOVE\n",
                "###\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {
                "collapsed": true,
                "nbgrader": {
                    "grade": true,
                    "grade_id": "Q1a:construct_dfB",
                    "locked": true,
                    "points": "2",
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "###\n",
                "### AUTOGRADER TEST - DO NOT REMOVE\n",
                "###\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "(b) Apply PCA to the movie-user matrix A with a specified number of components k. Return the resulting explained variance ratio vector and the transformed data (converted back to a pandas dataframe). "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "# apply_pca(df, k) takes as input a movie-user ratings dataframe and target number \n",
                "# of pca components k. The function should first mean center the data in the data \n",
                "# frame, them perform PCA with k components and transform the data. Return the pca\n",
                "# attribute explained_variance_ratio_ that records the explained variance for each \n",
                "# of the k components, and the transformed data (in a new data frame).\n",
                "\n",
                "def apply_pca(df, k):\n",
                "\n",
                "    ###\n",
                "    ### YOUR CODE HERE\n",
                "    ###\n",
                "\n",
                "df_A, df_B = construct_data('ratings.csv', 'movies.csv')\n",
                "apply_pca(df_A, 2)  # -> (array([0.1762..., 0.0418...]), transformed_df 9724x2)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {
                "collapsed": true,
                "nbgrader": {
                    "grade": true,
                    "grade_id": "Q1b:pca_explainedVar0",
                    "locked": true,
                    "points": "1",
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "###\n",
                "### AUTOGRADER TEST - DO NOT REMOVE\n",
                "###\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {
                "collapsed": true,
                "nbgrader": {
                    "grade": true,
                    "grade_id": "Q1b:pca_explainedVar1",
                    "locked": true,
                    "points": "2",
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "###\n",
                "### AUTOGRADER TEST - DO NOT REMOVE\n",
                "###\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {
                "collapsed": true,
                "nbgrader": {
                    "grade": true,
                    "grade_id": "Q1b:pca_transform",
                    "locked": true,
                    "points": "2",
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "###\n",
                "### AUTOGRADER TEST - DO NOT REMOVE\n",
                "###\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "(c) Join the data frames from Q1a (movie title and firstGenre) and Q1b (movies with dimensionality reduced by pca). The result should be a data frame with rows corresponding to movies, the first k columns should be the lower dimensional representation of user rating from pca, then the last three columns should be movieId, movie title, and firstGenre. Return the resulting data frame.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": [
                "# join_movieDataFrames(pcaDF, genreDF) takes two data frames as input: (1) movies \n",
                "# after dimensionalilty reduction with pca, and (2) movies with title and first genre.\n",
                "# The two data frames should be joined together, using movieId as a key, and returned \n",
                "# as a single data frame.\n",
                "\n",
                "def join_movieDataFrames(pcaDF, genreDF):\n",
                "\n",
                "    ###\n",
                "    ### YOUR CODE HERE\n",
                "    ###\n",
                "\n",
                "(df_A, df_B) = construct_data('ratings.csv', 'movies.csv')\n",
                "(exVar, df_T) = apply_pca(df_A, 2)\n",
                "join_movieDataFrames(df_T, df_B) # -> joined_df (9724x5)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {
                "collapsed": true,
                "nbgrader": {
                    "grade": true,
                    "grade_id": "Q1c:join_dfs",
                    "locked": true,
                    "points": "2",
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "###\n",
                "### AUTOGRADER TEST - DO NOT REMOVE\n",
                "###\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "(d) Consider the color mapping of each genre below in the dictionary 'genre_color' to assign a color to each genre. \n",
                "\n",
                "Now, use the methods above to apply PCA and reduce the dimensionality of the movies using k=2, join the results to movie title and firstGenre, then plot the results as a scattterplot, coloring each movie according to its firstGenre using the color map below. \n",
                "\n",
                "This question will be graded manually. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {
                "collapsed": true,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "genre_color={'Animation': 'r', 'Horror': 'b', 'Thriller': 'y', 'Drama': 'm', \n",
                "        'Comedy': 'deeppink', 'Sci-Fi': 'gold', 'Western': 'orange', 'Adventure': 'g', \n",
                "        'Documentary': 'brown', 'Musical': 'indigo', 'Fantasy': 'yellow', \n",
                "        'Mystery': 'purple', 'Film-Noir': 'cyan', '(no genres listed)': 'coral', \n",
                "        'Action': 'teal', 'War': 'black', 'Romance': 'skyblue', 'Children': 'lime', \n",
                "        'Crime': 'darkgreen'}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": [
                "###\n",
                "### YOUR CODE HERE\n",
                "###\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "## Q2: Correlations on document-term matrix"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "In the question below, you will construct a document-term matrix, which is a 2-dimensional matrix that describes the frequency of terms (i.e., words) that occur in each document in a collection. In a document-term matrix, rows correspond to documents in the collection and columns correspond to terms. The value in a given cell (i,j) counts the number of times the term j occurs in document i.\n",
                "Reference: https:\/\/en.wikipedia.org\/wiki\/Document-term_matrix"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "The text file \"urls.txt\" contains a list of urls for the webpages to be parsed. Each line in the text file corresponds to a url. Use BeautifulSoup to fetch each webpage and parse the text as you did in HW1. Specifically,\n",
                "1. For each webpage document, read the content and do the following:\n",
                "    A. Retrieve all text enclosed in paragraph tags.\n",
                "    B. Preprocess the text as follows:\n",
                "        i. Convert to lowercase. \n",
                "        ii. Strip out punctuation using translate() with string.punctuation.\n",
                "        iii. Tokenize based on whitespace separation. Note: use strip() to \n",
                "        remove all trailing whitespsace characters.\n",
                "    \n",
                "2. Find the set of unique words across all the documents and sort them in lexicographic order. This comprises the \"vocabulary\" of the corpus. Each term in the vocabulary will be a feature in the document-term matrix you will construct next. Do not add the empty string to the vocabulary. \n",
                "\n",
                "For the above two parts, you can use the functions\/code specified below, or you can use your code from previous homeworks, or you can write your own functions. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {
                "collapsed": true,
                "nbgrader": {
                    "grade": false,
                    "locked": false,
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "def preprocess(untokenizedWords):\n",
                "    \n",
                "    tokenizedWords = []\n",
                "    \n",
                "    for line in untokenizedWords:\n",
                "        text = line.translate(str.maketrans('','',string.punctuation))\n",
                "        text = text.strip().split();\n",
                "        for i in range(len(text)):\n",
                "            word = text[i].strip().lower();\n",
                "            tokenizedWords.append(word.strip());        \n",
                "    \n",
                "    return tokenizedWords"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "# Function to read the urls from the file and return them as a list.\n",
                "\n",
                "def getListOfUrls(filename):\n",
                "    \n",
                "    listOfUrls = [];\n",
                "    \n",
                "    with open(filename) as f:\n",
                "        for line in f:\n",
                "            url = line.split(\"\\n\")[0];\n",
                "            listOfUrls.append(url);\n",
                "    \n",
                "    return listOfUrls\n",
                "\n",
                "# getListOfUrls(\"urls.txt\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "# Function that parses each url and returns the list of toeknized and preprocessed words, \n",
                "# that are used to create the vocabulary next.\n",
                "\n",
                "def parse(listOfUrls):\n",
                "    \n",
                "    # list of tokenized (and preprocessed) words across all the documents\n",
                "    listOfWordsInDocuments = []; \n",
                "    \n",
                "    # parse each webpage (url)\n",
                "    for url in listOfUrls:   \n",
                "        page = requests.get(url)\n",
                "        soup = BeautifulSoup(page.content, 'html.parser')\n",
                "        # find all the paragraph tags \n",
                "        listOfp = soup.find_all('p') \n",
                "        # list of strings to accumulate all the content inside the different paragraph tags in a url.\n",
                "        listOfText = []  \n",
                "        for content in listOfp:\n",
                "            # get the content in each paragraph tag, as a string.\n",
                "            text = content.get_text(); \n",
                "            listOfText.append(text) \n",
                "        # preprocess the content inside each url to get a list of tokenized words.\n",
                "        words_doc = preprocess(listOfText) \n",
                "        # collect the tokenized words in each document.\n",
                "        listOfWordsInDocuments.append(words_doc); \n",
                "\n",
                "    return listOfWordsInDocuments"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "# Function to create the vocabulary that is returned as a list of words (strings). \n",
                "\n",
                "def createVocabulary(listOfWordsInDocuments):\n",
                "    \n",
                "    vocabulary = set() \n",
                "    for words_doc in listOfWordsInDocuments:\n",
                "        for word in words_doc:\n",
                "            if(word!=''): # check if the word is not an empty string.\n",
                "                vocabulary.add(word)    \n",
                "    vocabulary = list(vocabulary) \n",
                "    vocabulary = sorted(vocabulary);\n",
                "    \n",
                "    return vocabulary"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "(a) Now, create the document-term data frame, where the value of the cell (i,j) in the data frame is the frequency of the term j in document i. The number of columns of the document-term matrix will correspond to the number of unique words in the vocabulary, and the number of rows will correspond to the number of documents (i.e., webpages in urls.txt). Each term is represented by a column of the document-term data frame. Each document is represented by a row of the document-term dataframe (ordered by the order of the URLs given in the file). \n",
                "\n",
                "Return the document-term data frame obtained with columns as the vocabulary and index as the document id of the url, which is the order in which the urls appear, that is, first url in the file will have a document id of 1, second url will have a document id of 2, and so on. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [],
            "source": [
                "# createDocumentTermDataFrame(filename) takes as input the filenames to readin data from and \n",
                "# returns the document-term data frame as specified above.\n",
                "# You will use these dataframes in subsequent questions. \n",
                "\n",
                "# Example: \n",
                "# createDocumentTermDataFrame('urls.txt') -> df_doc_term \n",
                "import requests\n",
                "def createDocumentTermDataFrame(filename):\n",
                "    \n",
                "    ###\n",
                "    ### YOUR CODE HERE\n",
                "    ###\n",
                "    \n",
                "\n",
                "createDocumentTermDataFrame('urls.txt') # (3x1244 on 23 June 2020)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {
                "nbgrader": {
                    "grade": true,
                    "grade_id": "Q2a:doc_term_shape",
                    "locked": true,
                    "points": "2",
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "###\n",
                "### AUTOGRADER TEST - DO NOT REMOVE\n",
                "###\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "##### (b) Using the data frame from Q1a, consider the words: 'data', 'business', 'companies', 'mining', 'statistics', 'science'. Return a dictionary with the words as keys and the values as a list of their corresponding term frequencies for each document, (word: [list of frequencies]).\n",
                "\n",
                "Example:\n",
                "\n",
                "data\t[105.0, 0.0, 57.0]\n",
                "science\t[22.0, 16.0, 31.0]\n",
                "\n",
                "\n",
                "The above values are for the first three webpage urls given in the file \"urls.txt\". The frequencies should be listed in the order of the urls given in \"urls.txt\"."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [],
            "source": [
                "# getTermFrequencies(df_doc_term) takes as input the document-term data frame, and returns a dictionary with the above\n",
                "# specified words as keys and the values as a list of their corresponding term frequencies for each document.\n",
                "\n",
                "def getTermFrequencies(df_doc_term):\n",
                "    \n",
                "    ###\n",
                "    ### YOUR CODE HERE\n",
                "    ###\n",
                "    \n",
                "# Example:\n",
                "df_doc_term = createDocumentTermDataFrame('urls.txt')\n",
                "getTermFrequencies(df_doc_term)\n",
                "# Order may vary\n",
                "# {'business': [17.0, 0.0, 1.0],\n",
                "#  'companies': [1.0, 1.0, 2.0],\n",
                "#  'data': [105.0, 0.0, 57.0],\n",
                "#  'mining': [2.0, 0.0, 2.0],\n",
                "#  'science': [22.0, 16.0, 31.0],\n",
                "#  'statistics': [5.0, 0.0, 15.0]}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {
                "collapsed": true,
                "nbgrader": {
                    "grade": true,
                    "grade_id": "Q2b:term_frequencies",
                    "locked": true,
                    "points": "2",
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "###\n",
                "### AUTOGRADER TEST - DO NOT REMOVE\n",
                "###\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "(c) Calculate the correlation between the two documents here."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "# findCorrelation(filename, doc_1, doc_2) takes as input the file to read in data from and the document ids, \n",
                "# doc_1 and doc_2 of two documents, and returns the correlation between the documents with the document vector\n",
                "# as the frequencies of the words in the vocabulary.\n",
                "\n",
                "\n",
                "def findCorrelation(filename, doc_1, doc_2):\n",
                "    \n",
                "    ###\n",
                "    ### YOUR CODE HERE\n",
                "    ###\n",
                "    \n",
                "findCorrelation('urls.txt',1,2) # -> 0.6119..."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "nbgrader": {
                    "grade": true,
                    "grade_id": "Q2c:doc_correlation_0",
                    "locked": true,
                    "points": "1",
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "    ###\n",
                "    ### AUTOGRADER TEST - DO NOT REMOVE\n",
                "    ###\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {
                "nbgrader": {
                    "grade": true,
                    "grade_id": "Q2c:doc_correlation_1",
                    "locked": true,
                    "points": "1",
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "    ###\n",
                "    ### AUTOGRADER TEST - DO NOT REMOVE\n",
                "    ###\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "nbgrader": {
                    "grade": true,
                    "grade_id": "Q2c:doc_correlation_2",
                    "locked": true,
                    "points": "1",
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "###\n",
                "### AUTOGRADER TEST - DO NOT REMOVE\n",
                "###\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "(d) For every word, find its average frequency across all documents. Then create a new document-term data frame with the same dimensions as above. For the value of cell (i,j), set the value to +1 if the frequency of word j in document i is greater than the average frequency of word j, otherwise set it to -1. \n",
                "\n",
                "Return the transformed document-term data frame with the same columns and index as for the document-term data frame in Q2 a."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [],
            "source": [
                "# transformDocumentTermDataFrame(df_doc_term) takes as input the document term dataFrame df_doc_term obtained in Q2 a,\n",
                "# performs the above described transformation, and returns the transformed dataframe.\n",
                "\n",
                "\n",
                "def transformDocumentTermDataFrame(df_doc_term):\n",
                "    \n",
                "    ###\n",
                "    ### YOUR CODE HERE\n",
                "    ###\n",
                "\n",
                "df_doc_term = createDocumentTermDataFrame('urls.txt')\n",
                "transformDocumentTermDataFrame(df_doc_term) # 3x1244 on 23 June 2020, data should be transformed as described above."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {
                "nbgrader": {
                    "grade": true,
                    "grade_id": "Q2d:feat_transform0",
                    "locked": true,
                    "points": "1",
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "###\n",
                "### AUTOGRADER TEST - DO NOT REMOVE\n",
                "###\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {
                "nbgrader": {
                    "grade": true,
                    "grade_id": "Q2d:feat_transform1",
                    "locked": true,
                    "points": "1",
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "###\n",
                "### AUTOGRADER TEST - DO NOT REMOVE\n",
                "###\n"
            ]
        },
        {
            "cell_type": "raw",
            "metadata": {
                "collapsed": true,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "(e) Calculate the correlation again, but this time use the transformed data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [],
            "source": [
                "# findCorrelation_T(filename, doc_1, doc_2) takes as input the file to read in data from and the document ids, \n",
                "# doc_1 and doc_2 of two documents, and gets the transformed dataframe using the above function, and\n",
                "# returns the correlation between the documents with the document vector as the transformed values of the words in \n",
                "# the vocabulary.\n",
                "\n",
                "\n",
                "def findCorrelation_T(filename, doc_1, doc_2):\n",
                "    \n",
                "    ###\n",
                "    ### YOUR CODE HERE\n",
                "    ###\n",
                "\n",
                "findCorrelation_T('urls.txt', 1, 2) # -> -0.453... as of 23 June 2020"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {
                "nbgrader": {
                    "grade": true,
                    "grade_id": "Q2e:trans_correlation_0",
                    "locked": true,
                    "points": "1",
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "###\n",
                "### AUTOGRADER TEST - DO NOT REMOVE\n",
                "###\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {
                "nbgrader": {
                    "grade": true,
                    "grade_id": "Q2e:trans_correlation_1",
                    "locked": true,
                    "points": "1",
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "###\n",
                "### AUTOGRADER TEST - DO NOT REMOVE\n",
                "###\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {
                "nbgrader": {
                    "grade": true,
                    "grade_id": "Q2e:trans_correlation_2",
                    "locked": true,
                    "points": "1",
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "###\n",
                "### AUTOGRADER TEST - DO NOT REMOVE\n",
                "###\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "(f) Now, find the correlation between different documents and plot the correlation matrix as a heatmap. Note that you need to use the original document term dataframe obtained in Q2 a, and not the transformed one. \n",
                "\n",
                "Note that this question will be graded manually. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [],
            "source": [
                "###\n",
                "### YOUR CODE HERE\n",
                "###\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 [3.6]",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text\/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.4"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}